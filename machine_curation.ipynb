{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of machine_curation.ipynb","provenance":[{"file_id":"https://github.com/DurhamARC/machine-curation/blob/master/machine_curation.ipynb","timestamp":1629810111201}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z_yWWyyxXLSS"},"source":["# Machine Curation"]},{"cell_type":"markdown","metadata":{"id":"Swgs5dAzNFdf"},"source":["This notebook presents the code that underpins the machine-curated tour of the Liverpool Biennial collection. The machine curated exhibition can be viewed through the following [link](https://metaobjects.org/testing/liverpoolbiennial/).\n","\n","We load in data from GitHub and present the preprocessing steps that operate on that data and render it useful to our machine curator. Finally, we compute the similarity rankings across the Liverpool Biennial collection that ultimately allow viewers of the machine curated exhibition to navigate their way through the collection."]},{"cell_type":"markdown","metadata":{"id":"VfAMpVbrYeAl"},"source":["## First steps: getting the data"]},{"cell_type":"markdown","metadata":{"id":"hFtoAuGihGbm"},"source":["As a first step, we need to clone the repository that accompanies this notebook. It contains the data we will need in the steps below."]},{"cell_type":"code","metadata":{"id":"NMLis9zuhQPi"},"source":["!git clone https://github.com/DurhamARC/machine-curation.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Cfd9AnqQiwM"},"source":["Unzip the 50 images that comprise the Liverpool Biennial 2021 collection and load dataset."]},{"cell_type":"code","metadata":{"id":"D3HmpsjJQur-"},"source":["%pushd machine-curation/datasets/liverpool_biennial_2021/original_images\n","!unzip 'images_part_*.zip'\n","%popd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5SReDJgYSfg"},"source":["Load the metadata that describes the Liverpool Biennial collection and its content."]},{"cell_type":"code","metadata":{"id":"qSn2J2gDYal5"},"source":["import pandas as pd\n","lb2021 = pd.read_csv('machine-curation/datasets/liverpool_biennial_2021/LB2021_metadata.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45lGz32bZtCh"},"source":["# Preprocessing: generating data"]},{"cell_type":"markdown","metadata":{"id":"eZHgfY2IZ04u"},"source":["The next four sections create the data that our machine curator needs. We do this in four stages:\n","\n","\n","1.   A machine generated image is computed for each caption in the Liverpool Biennial.\n","2.   Keywords are extracted from the description of each artwork in the Liverpool Biennial.\n","3.   A machine generated caption is computed for each artwork in the Liverpool Biennial.\n","4.   Heatmaps are overlayed over each artwork in the Liverpool Biennial."]},{"cell_type":"markdown","metadata":{"id":"SFr3L_mc4AVM"},"source":["## Generate image from title"]},{"cell_type":"markdown","metadata":{"id":"Gi2CaVXPCj3A"},"source":["Each artwork in the Liverpool Biennial collection is associated with one machine generated image. This machine generated image is created by the `Imagine` model - a natural language to image model - from the [`big_sleep` module](https://github.com/lucidrains/big-sleep). That is, the code below maps artwork titles to machine generated images.\n","\n","First, we install the relevant modules."]},{"cell_type":"code","metadata":{"id":"PQFD71tq4vla"},"source":["!pip install folium==0.2.1 # an idiosyncracy of using pip with big-sleep in Colab\n","!pip install big-sleep"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g381GWKaVIqH"},"source":["To test out the model on one image caption (takes around 30 minutes) run the following code. It will prompt you for the name of an artwork which you should enter with an underscore separating each word.\n","\n","At the end you will we a file called `<given_title>.best.png` will have been generated. This is the image that the machine associates with your artwork title."]},{"cell_type":"code","metadata":{"id":"CHYB83GYVgdi"},"source":["import pandas as pd\n","from big_sleep import Imagine\n","\n","image_caption = input(\"Enter the name of an artwork in your collection with an \"\n","                      \"underscore separating each word, e.g. Masterless_Voices\")\n","\n","dream = Imagine(\n","    text = image_caption,\n","    lr = 5e-2,\n","    epochs = 1,\n","    iterations = 1000,\n","    save_every=200,\n","    num_cutouts = 32,\n","    save_best = True,\n",")\n","dream()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsHOmpjAA2wl"},"source":["As an example of the kind of output one  might get, let's consider the work **`Masterless Voices`** by Ines Doujak and John Barker. A description of this \n","artwork can be found [here](https://www.biennial.com/2021/exhibition/artists/ines-doujak-and-john-barker).\n","\n","When the string `Masterless_Voices` - the title of a work by Ines Doujak and John Barker - is entered, the following output is generated:\n","\n","![Machine_generated_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/Masterless_Voices.best.png)\n","\n","For comparison, below is the original artwork, the title of which is visualised by the ML model in the above.\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/Masterless_Voices.original.png)"]},{"cell_type":"markdown","metadata":{"id":"vOlrtBOehELw"},"source":["The following code allows us to run the model over all artwork titles in our dataset. The `*.best.png` files will collected and saved in a folder called `title_to_image_data`. \n","\n","This takes a very long time in Colab. To collect the data more quickly so it can be used in the stages below reduce the number of iterations passed to Imagine()."]},{"cell_type":"code","metadata":{"id":"nKqDPCdz4DFR","scrolled":false},"source":["import os\n","\n","import pandas as pd\n","from big_sleep import Imagine\n","\n","lb2021 = pd.read_csv('machine-curation/datasets/liverpool_biennial_2021/LB2021_metadata.csv')\n","\n","for item in lb2021.clean_title:\n","    dream = Imagine(\n","        text=item,\n","        lr=5e-2,\n","        epochs=1,\n","        save_every=200,\n","        save_progress=True,\n","        iterations=1000,\n","        num_cutouts=32,\n","        save_best=True,\n","    )\n","    dream()\n","\n","# Move 'best' machine generated images into a data dir\n","data_dir = \"title_to_image_data\"\n","os.mkdir(data_dir)\n","for file in os.listdir():\n","    if file.endswith(\".best.png\"):\n","        os.rename(file, os.path.join(data_dir, file))\n","    elif file.endswith(\".png\"):\n","        # clean cwdir\n","        os.remove(file)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f98w2iHMsTAl"},"source":["## Extract keywords\n"]},{"cell_type":"markdown","metadata":{"id":"tgA_IZZAEZxy"},"source":["Each artwork in the Liverpool Biennial is associated with detailed written literature discussing the themes, contexts and factual basis underlying the artwork. For example:"]},{"cell_type":"markdown","metadata":{"id":"DnVXq9mFArGG"},"source":["*Pan African Flag for the Relic Travellers' Alliance*, by Larry Achiampong .....\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/pan_african_flag.png)\n","\n"," ....... Maps to the following project description (from the Liverpool Biennial [website](https://www.liverpoolbiennial2021.com/))\n","\n","> \"Larry Achiampong presents a series of eight different Pan African flags, exhibited across ten locations, on buildings and streets throughout Liverpool city centre. With some designs featuring 54 stars that represent the 54 countries of Africa, the flags evoke solidarity and collective empathy – while some of their locations speak to Liverpool’s connection with the enslavement of West Africans as part of the transatlantic slave trade. The colours of the flags reflect Pan African symbolism: green, black and red represent Africa’s land, people and the struggles the continent has endured respectively, while yellow-gold represents a new dawn and prosperity. Achiampong has configured these colours into icons that are suggestive of community, motion and the human figure in ascension.For Liverpool Biennial 2021, four of the artist’s flags from his original series are shown - Ascension, Community, Motion and Squadron - as well as four new flag designs that generate new symbolic constitutions;What I hear I Keep – related to the act of sending and receiving messages that resonate.Dualities – related to the connection between those born within the African continent and those of the African Diaspora.Bringers of Life – related to the eternal reverence of the elements that bring and fortify life.Mothership – in praise, honour and respect of the centre of community; Black Womxn.Supported by The African Arts Trust. 'What I Hear I Keep' was commissioned by De La Warr Pavilion.This artwork is now open, local residents can plan your visit here.\""]},{"cell_type":"markdown","metadata":{"id":"TIB9msvjEwJL"},"source":["Our machine curator can extract keywords from the project descriptions for each artwork in the Liverpool Biennial. We demonstrate this step here. Subsequently, this will be used to facilitate transitions to images that resonate as similar with the keywords that we extract from the project descriptions in this step."]},{"cell_type":"markdown","metadata":{"id":"wceNFQdvsxui"},"source":["First, import the modules:"]},{"cell_type":"code","metadata":{"id":"L1yG3jvZsVJ4"},"source":["!pip install sentence_transformers\n","\n","import numpy as np\n","import pandas as pd\n","import itertools\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYgs7LY7s4XF"},"source":["Load the BERT model and read in our dataset (our workflow here follows [this](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea#_=_) example):"]},{"cell_type":"code","metadata":{"id":"Q8HxRwP5sViW"},"source":["model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n","\n","lb2021 = pd.read_csv('machine-curation/datasets/liverpool_biennial_2021/LB2021_metadata.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sfQzXyZMtMRT"},"source":["Define a diversification function:"]},{"cell_type":"code","metadata":{"id":"z1OAUENosV4U"},"source":["def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n","    # Calculate distances and extract keywords\n","    distances = cosine_similarity(doc_embedding, word_embeddings)\n","    distances_candidates = cosine_similarity(word_embeddings, \n","                                             word_embeddings)\n","\n","    # Get top_n words as candidates based on cosine similarity\n","    words_idx = list(distances.argsort()[0][-nr_candidates:])\n","    words_vals = [words[index] for index in words_idx]\n","    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n","\n","    # Calculate the combination of words that are the least similar to each other\n","    min_sim = np.inf\n","    candidate = None\n","    for combination in itertools.combinations(range(len(words_idx)), top_n):\n","        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n","        if sim < min_sim:\n","            candidate = combination\n","            min_sim = sim\n","\n","    return [words_vals[idx] for idx in candidate]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FSAgtlXrI5Et"},"source":["Define a function to extract keywords:"]},{"cell_type":"code","metadata":{"id":"yGop-n8tI9WR"},"source":["n_gram_range = (3, 3)\n","stop_words = \"english\"\n","\n","def extract_keywords(project_description):\n","    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([project_description])\n","    candidates = count.get_feature_names()\n","    doc_embedding = model.encode([project_description])\n","    candidate_embeddings = model.encode(candidates)\n","    return max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mz4OTjgZJrYA"},"source":["Now we can extract keywords from the example we looked at above - *Pan African Flag for the Relic Travellers' Alliance* by Larry Achiampong."]},{"cell_type":"code","metadata":{"id":"Kt3F2D5jKL0d"},"source":["# Extract the project description from our LB csv\n","project_description = lb2021.loc[lb2021.artist == \"larry-achiampong\", \"featured_text\"].values[0]\n","\n","# Extract keywords from project description\n","keywords = extract_keywords(project_description)\n","\n","print(\"The keywords for the 'Pan African Flag for the Relic Travellers' Alliance' project description are:\")\n","for count, keyword in enumerate(keywords):\n","    print(f\"  {count+1}) '{keyword}'\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ur4qC72Dg6oL"},"source":["There we have it - the keywords for the 'Pan African Flag for the Relic Travellers' Alliance' project description."]},{"cell_type":"markdown","metadata":{"id":"oXJKxCittoUe"},"source":["We can now use the same methodology to extract keywords from all LB2021 project descriptions:"]},{"cell_type":"code","metadata":{"id":"s83GwNb8toz0"},"source":["all_keywords = []\n","for project_description in lb2021.featured_text:\n","    all_keywords.append(extract_keywords(project_description))\n","\n","for index, keyword_set in enumerate(all_keywords):\n","    print(f\"The keywords for '{lb2021.iloc[index].clean_title}' by {lb2021.iloc[index].artist} are:\")\n","    for index, keyword in enumerate(keyword_set):\n","        print(f\"  {index+1}) '{keyword}'\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWKTEQlyw3hm"},"source":["## Generate image captions"]},{"cell_type":"markdown","metadata":{"id":"VRCONEeuaQqt"},"source":["Generating image captions using CATR"]},{"cell_type":"code","metadata":{"id":"PVPTQ6xlaauD"},"source":["!git clone https://github.com/saahiluppal/catr.git\n","%pushd catr/\n","!pip install -r requirements.txt\n","%popd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bd4--g32aqMA"},"source":["We can generate captions using the following syntax:\n","\n","`!python predict.py --path <path-to-png>`\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"Phw9QxGzayUp"},"source":["import os\n","import subprocess\n","\n","image_path = os.path.join(\n","    os.sep, \n","    'content', \n","    'machine-curation', \n","    lb2021.loc[lb2021.clean_title == 'Ammonite', 'path_to_original_image'].values[0]\n",")\n","output = subprocess.run(['python','catr/predict.py', '--path', image_path], capture_output=True)\n","print(f\"The machine generated caption for Ammonite is: '{output.stdout.decode('ascii').rstrip()}'\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHwD4Sc6SxeO"},"source":["For interest, here is a picture of the original artwork, *Ammonite*, by Alice Channer:\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/Ammonite.png)\n"]},{"cell_type":"markdown","metadata":{"id":"E1hBNq0H2nVY"},"source":["Now we can repeat the same process to get machine generated captions for all images in the Liverpool Biennial:"]},{"cell_type":"code","metadata":{"id":"_OS7No3y2miD"},"source":["machine_generated_captions = []\n","for index, artwork in enumerate(lb2021.clean_title):\n","    image_path = os.path.join(\n","        os.sep,\n","        'content',\n","        'machine-curation',\n","        lb2021.iloc[index].path_to_original_image\n","    )\n","    output = subprocess.run(['python','catr/predict.py', '--path', image_path], capture_output=True)\n","    machine_generated_captions.append(output.stdout.decode('ascii').rstrip())\n","    print(f\"The machine generated caption for '{artwork}' by {lb2021.iloc[index].artist} is: '{output.stdout.decode('ascii').rstrip()}'\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qrEHvZ_PxCa"},"source":["Our orignal machine curator uses the VLP model in this stage to generate captions. For more information about VLP see this [link](https://github.com/LuoweiZhou/VLP). This was preceded by feature extraction using [detectron-vlp](https://github.com/LuoweiZhou/detectron-vlp). We were, however, unable to reproduce this workflow in Colab given a number of dependecies that could not be fulfilled from within the Colab structure - for example, an old version of Torch and Cuda. CATR, used above, is a simple alternative with similar results."]},{"cell_type":"markdown","metadata":{"id":"0ehLf1eZ3TiD"},"source":["## Heatmaps"]},{"cell_type":"markdown","metadata":{"id":"DcZ1fTwf3XkZ"},"source":["Heatmaps are important in our machine-curated exhibition. They appear to the 'right' of the original image itself. The overlay is informed by the machine generated caption computed above. That is, the heatmap often highlights objects that appear in the machine's caption - it gives us a view into what a machine 'sees' in an artwork, the way it parses objects and interprets concepts."]},{"cell_type":"markdown","metadata":{"id":"Kdl7c_Ks3jYn"},"source":["First import the modules:"]},{"cell_type":"code","metadata":{"id":"EMgP1Yhb3YHy"},"source":["import os\n","import torch\n","!pip install git+https://github.com/openai/CLIP.git\n","import clip\n","from PIL import Image\n","import numpy as np\n","!pip install torchray\n","from torchray.attribution.grad_cam import grad_cam"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmU_ahtr3onJ"},"source":["And load the CLIP language model:"]},{"cell_type":"code","metadata":{"id":"3nnStqim3qfO"},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","def get_model():\n","    return clip.load(\"RN50\", device=device, jit=False)\n","model, preprocess = get_model()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_obYYmqp37OK"},"source":["Load the images"]},{"cell_type":"code","metadata":{"id":"hqqtbQ-W39S1"},"source":["images=[]\n","for index, artwork in enumerate(lb2021.clean_title):\n","    image_path = os.path.join(\n","        os.sep,\n","        'content',\n","        'machine-curation',\n","        lb2021.iloc[index].path_to_original_image\n","    )\n","    image = preprocess(Image.open(image_path).convert(\"RGB\"))\n","    images.append(image)\n","\n","image_input = torch.tensor(np.stack(images)).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BdhzRRN-ON_"},"source":["These functions are taken from miniClip ([here](https://github.com/HendrikStrobelt/miniClip/blob/main/miniclip/imageWrangle.py)) They allow us to compute heatmaps within Colab without having to install miniClip and all its dependencies (some of which create incompatibilities withour other code)"]},{"cell_type":"code","metadata":{"id":"BTdd01QA9KSI"},"source":["from matplotlib import cm\n","\n","def min_max_norm(array):\n","    lim = [array.min(), array.max()]\n","    array = array - lim[0] \n","    array.mul_(1 / (1.e-10+ (lim[1] - lim[0])))\n","    return array\n","\n","def torch_to_rgba(img):\n","    img = min_max_norm(img)\n","    rgba_im = img.permute(1, 2, 0).cpu()\n","    if rgba_im.shape[2] == 3:\n","        rgba_im = torch.cat((rgba_im, torch.ones(*rgba_im.shape[:2], 1)), dim=2)\n","    assert rgba_im.shape[2] == 4\n","    return rgba_im\n","\n","def numpy_to_image(img, size):\n","    \"\"\"\n","    takes a [0..1] normalized rgba input and returns resized image as [0...255] rgba image\n","    \"\"\"\n","    resized = Image.fromarray((img*255.).astype(np.uint8)).resize((size, size))\n","    return resized\n","\n","def heatmap(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\n","    if not size:\n","        size = image.shape[1]\n","\n","    img = torch_to_rgba(image).numpy() # [0...1] rgba numpy \"image\"\n","    hm = cm.hot(min_max_norm(heatmap).numpy()) # [0...1] rgba numpy \"image\"\n","\n","    img = np.array(numpy_to_image(img,size))\n","    hm = np.array(numpy_to_image(hm, size))\n","\n","    return Image.fromarray((alpha * hm + (1-alpha)*img).astype(np.uint8))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfwmerO8nAq5"},"source":["Now we can generate a heatmap overlay for all images in the Liverpool Biennial. These new images are saved in a new directory: `attention_maps`. Heatmaps are generated based on the machine generated captions computed above"]},{"cell_type":"code","metadata":{"id":"A1eLTqb1-Of0"},"source":["layer='layer4.2.relu'\n","alpha=0.7 # can be changed\n","\n","!rm -rf attention_maps\n","os.mkdir(\"attention_maps\")\n","\n","\n","att_img_path=[] \n","for i in range (len(lb2021)):\n","    image = image_input[i].reshape([1,3,224,224])\n","    txt_input = machine_generated_captions[i]\n","    tokenized_text = clip.tokenize(txt_input).to(device)\n","    with torch.no_grad():\n","        image_features = model.encode_image(image)\n","        text_features = model.encode_text(tokenized_text)\n","        image_features_norm = image_features.norm(dim=-1, keepdim=True)\n","        image_features_new = image_features / image_features_norm\n","        text_features_norm = text_features.norm(dim=-1, keepdim=True)\n","        text_features_new = text_features / text_features_norm\n","    text_prediction = (text_features_new* image_features_norm)\n","    saliency = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\n","    \n","    hm = heatmap(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)    \n","    img_name = os.path.join(\"attention_maps\", \"att_map_\"+lb2021.iloc[i].clean_title.replace(\" \", \"_\")+\".png\")\n","    att_img_path.append(img_name)\n","    hm.convert(\"RGB\").save(img_name)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5gFsLRzAi3z"},"source":["A few of the attention maps below are added reproduced here (you will see the rest in a folder called attention_maps after running the cell above). We map image captions to heatmaps showing the way objects from the captions are identified and highlighted by the heatmaps.\n","\n","\"A group of five different types of toothbrushes.\"\n","\n","(`Tongues` by Anu Põder)\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/att_map_Tongues.png)\n","\n","\"A car with a lot of surfboards in the back of it.\" \n","\n","(`Superposition` by Erick Beltrán\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/att_map_Superposition.png)\n","\n","\"A large building with a clock on the front of it\" \n","\n","(`Pan African Flag for the Relic Travellers' Alliance` by Larry Achiampong)\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/att_map_Pan_African_Flag_for_the_Relic_Travellers'_Alliance.png)\n","\n","\"A red liquid in a glass filled with liquid\" \n","\n","(`The Goblets` by Ane Graff)\n","\n","![original_image](https://raw.githubusercontent.com/DurhamARC/machine-curation/master/datasets/liverpool_biennial_2021/example_images/att_map_The_Goblets.png)"]},{"cell_type":"markdown","metadata":{"id":"tGQfakBUiG-g"},"source":["# CLIP similarities"]},{"cell_type":"markdown","metadata":{"id":"8mIQ9xSiLtn4"},"source":["Having completed the preprocessing steps above, we are now in a position to use this data to create connections between artworks in the collection. These connections are based on similarity values and inform way viewers can use the machine curator to move between images. There are three steps to this workflow:\n","\n","\n","1.   The similarities between captions are ascertained.\n","2.   The similarities between machine-generated images are computed\n","3.   The similarities between artworks and computed keywords are computed.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MTBPZXf1XTNq"},"source":["First, we import modules"]},{"cell_type":"code","metadata":{"id":"VgV9LN7pXZEY"},"source":["import json\n","\n","import torch\n","import clip\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","from scipy.spatial import distance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VDz-iyrIXrC3"},"source":["Load the CLIP model\n"]},{"cell_type":"code","metadata":{"id":"AjfGk7uZXp4f"},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Working with {device}\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBKsT0FBbAhq"},"source":["## Caption similarities"]},{"cell_type":"markdown","metadata":{"id":"2Av3ZBcAX-rp"},"source":["Now we are in a position to be able to compute the similarity values between the machine generated captions that belong to each artwork in the Liverpool Biennial collection. These similarity values are vased on the cosine distance of the CLIP text features.\n","\n","In the machine curated exhibition this data is used when navigating right. That is, when moving to the right the viewer is shown a picture with the most similar machine generated caption to the artwork's caption that is currently being viewed."]},{"cell_type":"code","metadata":{"id":"oiOGZPgbX-AJ"},"source":["txt_input = machine_generated_captions\n","text = clip.tokenize(txt_input).to(device)\n","\n","with torch.no_grad():\n","    Caption_features = model.encode_text(text)\n","Caption_features /= Caption_features.norm(dim=-1, keepdim=True)\n","\n","# calculate the ordering of indices and distance values based on the distance between caption features \n","CaptionTxtDistance_ind=np.argsort(distance.cdist(Caption_features.cpu().numpy(),Caption_features.cpu().numpy(), metric='cosine')).squeeze()\n","CaptionTxtDistance_val=np.sort(distance.cdist(Caption_features.cpu().numpy(),Caption_features.cpu().numpy(), metric='cosine')).squeeze()\n","\n","# save list of tuples (index, similarity value)\n","CaptionTxtDistance=[]\n","for i in range (len(lb2021)):\n","    tup=[]\n","    for j in range (1,len(lb2021)):\n","        tup.append((int(CaptionTxtDistance_ind[i][j]), float(1.0-CaptionTxtDistance_val[i][j]))) #similarity=1-distance\n","    CaptionTxtDistance.append(tup)\n","\n","print(CaptionTxtDistance)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJ2huDQLZYsz"},"source":["Now let us look at the similarities for the following artwork:"]},{"cell_type":"code","metadata":{"id":"hZFHwGiEJr_g"},"source":["print(lb2021.iloc[0].clean_title)\n","print(machine_generated_captions[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0v6ULhiKZe7y"},"source":["similarities to the above are recorded here:"]},{"cell_type":"code","metadata":{"id":"64Sjol7GZfRF"},"source":["print(CaptionTxtDistance[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMEX_pjuZpZi"},"source":["The most dissimilar caption is:"]},{"cell_type":"code","metadata":{"id":"9t-MaS96ZrvB"},"source":["most_dissimilar_tuple = CaptionTxtDistance[0][-1]\n","print(most_dissimilar_tuple)\n","print(machine_generated_captions[most_dissimilar_tuple[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"difNLldHZvy_"},"source":["The most similar caption is:"]},{"cell_type":"code","metadata":{"id":"BZGzxYMjZyuq"},"source":["most_similar_tuple = CaptionTxtDistance[0][0]\n","print(most_similar_tuple)\n","print(machine_generated_captions[most_similar_tuple[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TgEI4u3bER7"},"source":["## Similarities between generated images"]},{"cell_type":"markdown","metadata":{"id":"Qj6CwDcyBNRk"},"source":["Here we get similarity values between artworks based on the cosine distance of the CLIP image features (where image=generated images).\n","\n","This is used in our machine-curated exhibition when navigating left. That is, to the left of the original image, we show a computer generated image; clicking on that image leads the viewer to the most similar generated image in the collection."]},{"cell_type":"code","metadata":{"id":"uNIOKEs9Babg"},"source":["#load (from title) generated images and extract image features\n","\n","gen_images=[]\n","for item in lb2021.generated_img_path:\n","    image = preprocess(Image.open(item).convert(\"RGB\"))\n","    gen_images.append(image)\n","\n","image_input = torch.tensor(np.stack(gen_images))\n","with torch.no_grad():\n","    gen_images_features = model.encode_image(image_input).float()\n","\n","gen_images_features /= gen_images_features.norm(dim=-1, keepdim=True)\n","\n","# calculate the ordering of indices and distance values based on the distance between generated image features \n","gen_images_distance_ind = np.argsort(distance.cdist(gen_images_features.cpu().numpy(),gen_images_features.cpu().numpy(), metric='cosine')).squeeze()\n","gen_images_distance_val = np.sort(distance.cdist(gen_images_features.cpu().numpy(),gen_images_features.cpu().numpy(), metric='cosine')).squeeze()\n","\n","# save list of tuples (index, similarity value)\n","gen_images_distance=[]\n","for i in range (len(lb2021)):\n","    tup=[]\n","    for j in range (1,len(lb2021)):\n","        tup.append((int(gen_images_distance_ind[i][j]), float(1.0-gen_images_distance_val[i][j]))) #similarity=1-distance\n","    gen_images_distance.append(tup)\n","\n","print(gen_images_distance)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNj-DE1j0s-I"},"source":["## Similarities between artworks and computed keywords"]},{"cell_type":"markdown","metadata":{"id":"8Ny-km5lgOxx"},"source":["Here we compute similarity values between artworks based on the cosine distance of the joint CLIP image and text features. By image we refer to the original artworks and by text we signify the keywords that were extracted from artwork descriptions by a BERT language model.\n","\n","This step enables the machine curator to determine which images resonate with another image's keywords. In the exhibition this is used when navigating using the down arrow."]},{"cell_type":"code","metadata":{"id":"6pROZn0tgQCA"},"source":["# load keywords and extract text features\n","text = clip.tokenize(machine_generated_captions).to(device)\n","\n","with torch.no_grad():\n","    keywords_features = model.encode_text(text)\n","keywords_features /= keywords_features.norm(dim=-1, keepdim=True)\n","\n","# load original artwork images and extract image features\n","orginal_images=[]\n","for img in lb2021.path_to_original_image:\n","    img_path = os.path.join('machine-curation', img)\n","    image = preprocess(Image.open(img_path).convert(\"RGB\"))\n","    orginal_images.append(image)\n","\n","image_input = torch.tensor(np.stack(orginal_images))\n","with torch.no_grad():\n","    orginal_images_features = model.encode_image(image_input).float()\n","orginal_images_features /= orginal_images_features.norm(dim=-1, keepdim=True)\n","\n","# joint features from image and text features\n","joint_features = (orginal_images_features+keywords_features)/2\n","\n","# calculate the ordering of indices and distance values based on the distance between joint image and text features\n","joint_features_distance_ind=np.argsort(distance.cdist(joint_features.cpu().numpy(),joint_features.cpu().numpy(),metric='cosine')).squeeze()\n","joint_features_distance_val=np.sort(distance.cdist(joint_features.cpu().numpy(),joint_features.cpu().numpy(),metric='cosine').squeeze())\n","\n","# save list of tuples (index, similarity value)\n","joint_features_distance=[]\n","for i in range (len(df)):\n","    tup=[]\n","    for j in range (1,len(df)):\n","        tup.append((int(JointFeaturesDistance_ind[i][j]), float(1.0-JointFeaturesDistance_val[i][j]))) #similarity=1-distance\n","    joint_features_distance.append(tup)\n","\n","print(joint_features_distance)"],"execution_count":null,"outputs":[]}]}